{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "852251da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "505e6cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_eeg_path = glob('Music Thinking/sub-*/ses-*/eeg/*.set')\n",
    "len(all_eeg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24273890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "behaviour_data = np.genfromtxt('Music Thinking/stimuli/Behavioural_data.txt')\n",
    "behaviour_data = behaviour_data[1:]\n",
    "\n",
    "labels = behaviour_data[:,2]\n",
    "for i in range(labels.shape[0]):\n",
    "    # Enjoyment level 1 or 2 -> enjoy the most\n",
    "    if labels[i] <= 2:\n",
    "        labels[i] = 1\n",
    "        \n",
    "    # Neutral or does not enjoy (enjoyment levels 3 to 5)\n",
    "    else:\n",
    "        labels[i] = 0\n",
    "        \n",
    "# The proportion of people liking/disliking the music is relatively the same.\n",
    "print(np.count_nonzero(labels == 1)) # LIKE\n",
    "print(np.count_nonzero(labels == 0)) # DISLIKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23844f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "# Associate each path to the corresponding class (binary classification)\n",
    "like_path = []\n",
    "dislike_path = []\n",
    "\n",
    "for i in range(labels.shape[0]):\n",
    "    if labels[i] == 1:\n",
    "        like_path = np.append(like_path, all_eeg_path[i])\n",
    "    else:\n",
    "        dislike_path = np.append(dislike_path, all_eeg_path[i])\n",
    "\n",
    "## TEST\n",
    "# 20 INDIVIDUALS FOR TESTING\n",
    "\n",
    "like_path = like_path[0:10]\n",
    "dislike_path = dislike_path[0:10]\n",
    "        \n",
    "print(like_path.shape[0], dislike_path.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ff9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that reads EEG Data (set extension file)\n",
    "def read_set_data(path):\n",
    "    music_data = mne.io.read_raw_eeglab(path, preload = True)\n",
    "    #music_data.set_eeg_reference()\n",
    "    \n",
    "    epochs = mne.make_fixed_length_epochs(music_data, duration = 3, overlap = 2, preload = True)\n",
    "    music_array = epochs.get_data()\n",
    "    music_array = music_array[:,:,:750]\n",
    "    \n",
    "    number_epochs = 4\n",
    "    array_epochs = np.empty(number_epochs, dtype = object)\n",
    "\n",
    "    for i in range(number_epochs):\n",
    "        chosen_number = random.randint(0, music_array.shape[0]-1)\n",
    "        print(chosen_number)\n",
    "        array_epochs[i] = music_array[chosen_number]\n",
    "    \n",
    "    return array_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b71c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "random.seed(0)\n",
    "\n",
    "like_epoch_array = np.empty((len(like_path)), dtype = object)\n",
    "dislike_epoch_array = np.empty((len(dislike_path)), dtype = object)\n",
    "\n",
    "for i in range(len(like_path)):\n",
    "    like_epoch_array[i] = read_set_data(like_path[i])\n",
    "    \n",
    "for i in range(len(dislike_path)):\n",
    "    dislike_epoch_array[i] = read_set_data(dislike_path[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec0177fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "# Assign the labels for each epoch\n",
    "like_epoch_labels = np.empty((len(like_path)), dtype = object)\n",
    "dislike_epoch_labels = np.empty((len(dislike_path)), dtype = object)\n",
    "\n",
    "for i in range(len(like_path)):\n",
    "    like_epoch_labels[i] = len(like_epoch_array[i]) * [1]\n",
    "    \n",
    "for i in range(len(dislike_path)):\n",
    "    dislike_epoch_labels[i] = len(dislike_epoch_array[i]) * [0]\n",
    "    \n",
    "print(len(like_epoch_labels), len(dislike_epoch_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fedd9cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dislike_epoch_array[6][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3a8bc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.hstack(np.append(like_epoch_array[0:6], dislike_epoch_array[0:6]))\n",
    "X_val = np.hstack(np.append(like_epoch_array[6:], dislike_epoch_array[6:]))\n",
    "\n",
    "y_train = np.hstack(np.append(like_epoch_labels[0:6], dislike_epoch_labels[0:6]))\n",
    "y_val = np.hstack(np.append(like_epoch_labels[6:], dislike_epoch_labels[6:]))\n",
    "\n",
    "X_train[:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7616628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 1, 129, 750) (32, 1, 129, 750) (48,) (32,)\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = np.zeros((X_train.shape[0], 1, 129, 750))\n",
    "X_val_tensor = np.zeros((X_val.shape[0], 1, 129, 750))\n",
    "\n",
    "for i in range(X_train_tensor.shape[0]):\n",
    "    X_train_tensor[i,0,:,:] = X_train[i]\n",
    "    \n",
    "for i in range(X_val_tensor.shape[0]):\n",
    "    X_val_tensor[i,0,:,:] = X_val[i]\n",
    "\n",
    "print(X_train_tensor.shape, X_val_tensor.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22fb9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGTrain(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #data loading\n",
    "        self.x = torch.from_numpy(X_train_tensor).float()\n",
    "        self.y = torch.from_numpy(y_train).long()\n",
    "        self.n_samples = len(y_train)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96d65b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGVal(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #data loading\n",
    "        self.x = torch.from_numpy(X_val_tensor).float()\n",
    "        self.y = torch.from_numpy(y_val).long()\n",
    "        self.n_samples = len(y_val)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "55aa4403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TESTING\n",
    "# Batch-size: let's test with 4 for now\n",
    "train_data = EEGTrain()\n",
    "train_dl = DataLoader(dataset = train_data, batch_size = 4, shuffle = True)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dl))\n",
    "print(train_features.size(), train_labels.size())\n",
    "\n",
    "train_labels\n",
    "\n",
    "num_epochs = 2\n",
    "total_samples = len(train_data)\n",
    "n_iterations = math.ceil(total_samples/4)\n",
    "print(n_iterations)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_dl):\n",
    "        #forward and backward pass, update our weights\n",
    "        print(f'epoch {epoch + 1}/{num_epochs}, step {i + 1}/{n_iterations}, inputs {inputs.shape}')\n",
    "        print(labels)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f1503b",
   "metadata": {},
   "source": [
    "## ATTEMPT FOR RNN IMPLEMENTATION HAHA... :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "332cc5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "hidden_size = 128\n",
    "num_classes = 2\n",
    "num_epochs = 1\n",
    "batch_size = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 129\n",
    "sequence_length = 750\n",
    "num_layers = 2\n",
    "\n",
    "# DATASETS\n",
    "train_data = EEGTrain()\n",
    "train_dl = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "val_data = EEGVal()\n",
    "val_dl = DataLoader(dataset = val_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c6eaf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK\n",
    "\n",
    "# Vanilla RNN\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        # x -> (batch_size, sequence_length, input_size)\n",
    "        \n",
    "        # or:\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # out -> (batch_size, sequence_length, hidden_size)\n",
    "        # out -> (N, 129, 128) ->> NOTE: CHANGE SEQUENCE_LENGTH AS 750 LATER (TRANPOSE THE TENSOR)\n",
    "        out = out[:, -1, :]\n",
    "        # out -> (N, 128)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "872f4b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our Vanilla RNN, we have only two layers, this code\n",
    "# wants to only freeze the first layer of the RNN.\n",
    "counter = 0\n",
    "for child in model.children():\n",
    "    counter += 1\n",
    "    if counter == 1:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2b8e06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): RNN(129, 128, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIRST TEST FOR TRANSFER LEARNING\n",
    "\n",
    "# If we want to freeze all the parameters except the fully connected layer\n",
    "# for param in model.parameters():\n",
    "#    print('test')\n",
    "#    param.requires_grad = False\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Linear(num_features, 2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "160e656a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1/10], Loss: 0.6880\n",
      "Epoch [1/1], Step [2/10], Loss: 0.7019\n",
      "Epoch [1/1], Step [3/10], Loss: 0.7013\n",
      "Epoch [1/1], Step [4/10], Loss: 0.6998\n",
      "Epoch [1/1], Step [5/10], Loss: 0.7071\n",
      "Epoch [1/1], Step [6/10], Loss: 0.6959\n",
      "Epoch [1/1], Step [7/10], Loss: 0.6892\n",
      "Epoch [1/1], Step [8/10], Loss: 0.6924\n",
      "Epoch [1/1], Step [9/10], Loss: 0.6940\n",
      "Epoch [1/1], Step [10/10], Loss: 0.6919\n",
      "tensor([1, 1, 1, 1, 1]) tensor([1, 0, 1, 0, 1])\n",
      "tensor([1, 1, 1, 1, 1]) tensor([1, 1, 1, 0, 1])\n",
      "tensor([1, 1, 1, 1, 1]) tensor([1, 0, 0, 1, 0])\n",
      "tensor([1, 1, 1, 1, 1]) tensor([1, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1]) tensor([0, 0, 1, 1, 0])\n",
      "tensor([1, 1, 1, 1, 1]) tensor([1, 0, 1, 0, 1])\n",
      "tensor([1, 1]) tensor([0, 1])\n",
      "Accuracy of the network: 50.0 %\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Train the model\n",
    "n_total_steps = len(train_dl)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_dl):  \n",
    "        # origin shape: [N, 1, 28, 28]\n",
    "        # resized: [N, 28, 28]\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        \n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in val_dl:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        print(predicted, labels)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a78cc05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a908b1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1/10], Loss: 0.7248\n",
      "Epoch [1/1], Step [2/10], Loss: 0.7095\n",
      "Epoch [1/1], Step [3/10], Loss: 0.6791\n",
      "Epoch [1/1], Step [4/10], Loss: 0.6766\n",
      "Epoch [1/1], Step [5/10], Loss: 0.6746\n",
      "Epoch [1/1], Step [6/10], Loss: 0.7439\n",
      "Epoch [1/1], Step [7/10], Loss: 0.6738\n",
      "Epoch [1/1], Step [8/10], Loss: 0.7356\n",
      "Epoch [1/1], Step [9/10], Loss: 0.7762\n",
      "Epoch [1/1], Step [10/10], Loss: 0.7236\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "n_total_steps = len(train_dl)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_dl):  \n",
    "        # origin shape: [N, 1, 28, 28]\n",
    "        # resized: [N, 28, 28]\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a40f4b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1]) tensor([1, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1]) tensor([1, 1, 1, 0, 1])\n",
      "tensor([1, 1, 1, 1, 1]) tensor([0, 1, 0, 0, 1])\n",
      "tensor([1, 1, 1, 1, 1]) tensor([0, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1]) tensor([1, 0, 0, 1, 0])\n",
      "tensor([1, 1, 1, 1, 1]) tensor([1, 1, 0, 0, 0])\n",
      "tensor([1, 1]) tensor([1, 0])\n",
      "Accuracy of the network: 50.0 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in val_dl:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        print(predicted, labels)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DOCUMENTATION:\n",
    "\n",
    "Accomplished tasks:\n",
    "-> Set datasets and dataloaders for Pytorch Neural networks' inputs\n",
    "-> Testing our very first RNN model ever with a very small sample_size (20 individuals to be exact)\n",
    "\n",
    "Upcoming steps...\n",
    "-> Transpose the matrix 129-750 (129 electrodes x 750 time points).\n",
    "-> Experiment and add new layers in RNN (add dropout rate, etc.). The model implemented was only one hidden layer.\n",
    "-> Experiment with a larger sample size in the cluster.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
